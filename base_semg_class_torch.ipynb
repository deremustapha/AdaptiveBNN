{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## USING CONV2D and CONV1D"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45459c26ee2fc00a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:25:49.809393Z",
     "start_time": "2024-03-26T21:25:47.766071Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from bmis_emg_utils import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subject = 6\n",
    "no_gesture = 7\n",
    "fs = 200\n",
    "notch_freq = 60.0\n",
    "quality_factor = 30.0\n",
    "fc = 10.0\n",
    "fh = 99.0\n",
    "order = 5\n",
    "window_time = 200\n",
    "overlap = 60\n",
    "no_channel = 8 # use 1 for 1D Conv and 8 for 2D Conv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:25:52.181878Z",
     "start_time": "2024-03-26T21:25:52.176344Z"
    }
   },
   "id": "3cc1f648236943f5",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total data shape is (4148, 8, 40) and label is (4148, 1)\n",
      "Training Set is(3318, 1, 8, 40) Test Set (830, 1, 8, 40)\n",
      "Training Label Set is(3318,) Test Label Set (830,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "data, label = get_data_subject_specific(subject, no_gesture, fs, notch_freq, quality_factor, fc, fh, order, window_time, overlap, no_channel)\n",
    "\n",
    "print('The total data shape is {} and label is {}'.format(data.shape, label.shape))\n",
    "\n",
    "#np.squeeze(data)\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = spilt_data(data, label, ratio=0.2)\n",
    "\n",
    "y_train = np.squeeze(y_train, axis=1)\n",
    "y_test = np.squeeze(y_test, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
    "X_test = np.transpose(X_test, (0, 3, 1, 2))\n",
    "\n",
    "\n",
    "print('Training Set is{} Test Set {}'.format(X_train.shape, X_test.shape))\n",
    "\n",
    "print('Training Label Set is{} Test Label Set {}'.format(y_train.shape, y_test.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:26:21.598984Z",
     "start_time": "2024-03-26T21:26:21.520055Z"
    }
   },
   "id": "53251791cd192a78",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.from_numpy(data).to(torch.float32)\n",
    "        self.labels = torch.from_numpy(labels).to(torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:47:46.726147Z",
     "start_time": "2024-03-26T21:47:46.719719Z"
    }
   },
   "id": "cc8c686590555031",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:47:50.767110Z",
     "start_time": "2024-03-26T21:47:50.759567Z"
    }
   },
   "id": "889027d4cc639293",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([16, 1, 8, 40]) torch.Size([16])\n",
      "torch.Size([6, 1, 8, 40]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(train_loader):\n",
    "    \n",
    "    print(X.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:47:56.122138Z",
     "start_time": "2024-03-26T21:47:55.833523Z"
    }
   },
   "id": "7a39ad3da6e1ed58",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)  # Change the number of input channels to 1\n",
    "        self.pool = nn.MaxPool2d(2, 2) # 2x2 max pooling\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3)\n",
    "        self.fc1 = nn.Linear(1152, 151)\n",
    "        self.fc3 = nn.Linear(151, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # 2x2 max pooling after first convolutional layer\n",
    "        x = self.pool(F.relu(self.conv2(x))) # 2x2 max pooling after second convolutional layer\n",
    "\n",
    "        # Flatten (reshape to 1D vector of size 16*5*5=400 for batch size 32)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-07)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:48:01.887949Z",
     "start_time": "2024-03-26T21:48:01.872657Z"
    }
   },
   "id": "77dcbf9d06e92e2e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, 3)  # Change the number of input channels to 1\n",
    "        self.pool = nn.MaxPool1d(1, 1) # 2x2 max pooling\n",
    "        self.conv2 = nn.Conv1d(32, 32, 3)\n",
    "        self.fc1 = nn.Linear(1152, 151)\n",
    "        self.fc3 = nn.Linear(151, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # 2x2 max pooling after first convolutional layer\n",
    "        x = self.pool(F.relu(self.conv2(x))) # 2x2 max pooling after second convolutional layer\n",
    "\n",
    "        # Flatten (reshape to 1D vector of size 16*5*5=400 for batch size 32)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-07)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9d87a1f3ae81dfb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        accuracy = (pred.argmax(1) == y).type(torch.float).sum().item() / len(y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "\n",
    "            print(f\"Training Accuracy: {accuracy*100:>5f}%\")\n",
    "            \n",
    "            \n",
    "            print(f\"Training loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating th e model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "\n",
    "\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            _, preds = torch.max(pred , 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return all_preds, all_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:48:08.974036Z",
     "start_time": "2024-03-26T21:48:08.961230Z"
    }
   },
   "id": "7595b18aeacf0d02",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training Accuracy: 18.750000%\n",
      "Training loss: 1.946236  [   16/ 3318]\n",
      "Training Accuracy: 43.750000%\n",
      "Training loss: 1.738241  [ 1616/ 3318]\n",
      "Training Accuracy: 62.500000%\n",
      "Training loss: 1.519277  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.467238 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training Accuracy: 75.000000%\n",
      "Training loss: 1.404920  [   16/ 3318]\n",
      "Training Accuracy: 81.250000%\n",
      "Training loss: 1.347493  [ 1616/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.235177  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 1.408822 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training Accuracy: 81.250000%\n",
      "Training loss: 1.328267  [   16/ 3318]\n",
      "Training Accuracy: 75.000000%\n",
      "Training loss: 1.442610  [ 1616/ 3318]\n",
      "Training Accuracy: 68.750000%\n",
      "Training loss: 1.471652  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 1.387416 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.208263  [   16/ 3318]\n",
      "Training Accuracy: 81.250000%\n",
      "Training loss: 1.377410  [ 1616/ 3318]\n",
      "Training Accuracy: 68.750000%\n",
      "Training loss: 1.461151  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 1.387669 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training Accuracy: 81.250000%\n",
      "Training loss: 1.360295  [   16/ 3318]\n",
      "Training Accuracy: 75.000000%\n",
      "Training loss: 1.436936  [ 1616/ 3318]\n",
      "Training Accuracy: 68.750000%\n",
      "Training loss: 1.531642  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 1.381690 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training Accuracy: 81.250000%\n",
      "Training loss: 1.373852  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.190537  [ 1616/ 3318]\n",
      "Training Accuracy: 87.500000%\n",
      "Training loss: 1.311446  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 1.305755 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.223237  [   16/ 3318]\n",
      "Training Accuracy: 87.500000%\n",
      "Training loss: 1.287450  [ 1616/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.220341  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 1.258110 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.176004  [   16/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.239083  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.184776  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 1.247301 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training Accuracy: 87.500000%\n",
      "Training loss: 1.283954  [   16/ 3318]\n",
      "Training Accuracy: 87.500000%\n",
      "Training loss: 1.263103  [ 1616/ 3318]\n",
      "Training Accuracy: 87.500000%\n",
      "Training loss: 1.273983  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 1.242756 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.228994  [   16/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.232298  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.179062  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 93.1%, Avg loss: 1.236210 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.168389  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.187898  [ 1616/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.236074  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 1.250783 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.228639  [   16/ 3318]\n",
      "Training Accuracy: 87.500000%\n",
      "Training loss: 1.308126  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.187438  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 1.230252 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.199773  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.188583  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.167154  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 92.8%, Avg loss: 1.239133 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.166677  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165441  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.172391  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 1.234187 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training Accuracy: 87.500000%\n",
      "Training loss: 1.287221  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.166786  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.172079  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 1.243165 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.226367  [   16/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.232773  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.166998  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 1.227003 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165434  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.166263  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165739  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 1.225178 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165534  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165602  [ 1616/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.218375  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 1.242709 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.226422  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.168769  [ 1616/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.227992  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 94.2%, Avg loss: 1.226452 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165432  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.175128  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.176949  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 93.7%, Avg loss: 1.230466 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.181244  [   16/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.223025  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.167127  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 1.239660 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.223633  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165613  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165435  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 94.1%, Avg loss: 1.224935 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165445  [   16/ 3318]\n",
      "Training Accuracy: 87.500000%\n",
      "Training loss: 1.272577  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165495  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 93.7%, Avg loss: 1.226551 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165448  [   16/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.231037  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165427  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 1.229995 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.227716  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165482  [ 1616/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.227911  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 1.225065 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165426  [   16/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.227949  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165934  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 1.231183 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.227851  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.179277  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165533  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 1.218195 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165429  [   16/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.227977  [ 1616/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165422  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 1.216959 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165449  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.166072  [ 1616/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.227906  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 1.219953 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.166020  [   16/ 3318]\n",
      "Training Accuracy: 100.000000%\n",
      "Training loss: 1.165600  [ 1616/ 3318]\n",
      "Training Accuracy: 93.750000%\n",
      "Training loss: 1.216914  [ 3216/ 3318]\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 1.219058 \n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, model, loss_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:48:46.614304Z",
     "start_time": "2024-03-26T21:48:11.298444Z"
    }
   },
   "id": "84c5e379b8048fd5",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_labels, all_preds = test_loop(test_loader, model, loss_fn)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(7)  # Assuming 7 classes\n",
    "plt.xticks(tick_marks, range(7), rotation=45)\n",
    "plt.yticks(tick_marks, range(7))\n",
    "\n",
    "# Loop over data dimensions and create text annotations\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"red\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b983eeb70b42253e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader.dataset.labels.dtype\n",
    "train_loader.dataset.data.dtype"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5cec51e0058efad",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CROSS SUBJECT EVALUATION"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7ff457f685f2752"
  },
  {
   "cell_type": "markdown",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86aa7811ad92920a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "subject = 2\n",
    "no_gesture = 7\n",
    "fs = 200\n",
    "notch_freq = 60.0\n",
    "quality_factor = 30.0\n",
    "fc = 10.0\n",
    "fh = 99.0\n",
    "order = 5\n",
    "window_time = 200\n",
    "overlap = 60\n",
    "no_channel = 8\n",
    "\n",
    "\n",
    "data_2, label_2 = get_data_subject_specific(subject, no_gesture, fs, notch_freq, quality_factor, fc, fh, order, window_time, overlap, no_channel)\n",
    "\n",
    "X_train_2, y_train_2, X_test_2, y_test_2 = spilt_data(data_2, label_2, ratio=0.2)\n",
    "\n",
    "y_train_2 = np.squeeze(y_train_2, axis=1)\n",
    "y_test_2 = np.squeeze(y_test_2, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_2 = np.expand_dims(X_train_2, axis=3)\n",
    "X_test_2 = np.expand_dims(X_test_2, axis=3)\n",
    "\n",
    "X_train_2 = np.transpose(X_train_2, (0, 3, 1, 2))\n",
    "X_test_2 = np.transpose(X_test_2, (0, 3, 1, 2))\n",
    "\n",
    "\n",
    "print('Training Set is{} Test Set {}'.format(X_train.shape, X_test.shape))\n",
    "\n",
    "# Create a TensorDataset object\n",
    "train_dataset = TensorDataset(X_train_2, y_train_2)\n",
    "test_dataset = TensorDataset(X_test_2, y_test_2)\n",
    "\n",
    "# Create a DataLoader object\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bbac3b8102acfc8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(2, 50)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.arange(100).reshape(2, 50)\n",
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:54:49.604814Z",
     "start_time": "2024-03-26T21:54:49.601376Z"
    }
   },
   "id": "331697fa307306f9",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "overlap = 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:55:14.557496Z",
     "start_time": "2024-03-26T21:55:14.551022Z"
    }
   },
   "id": "cbe41fc5d482ee69",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sliding_window(data, window_size, overlap):\n",
    "    # Number of channels is the number of rows in the data\n",
    "    num_channels = data.shape[0]\n",
    "\n",
    "    # Number of datapoints is the number of columns in the data\n",
    "    num_datapoints = data.shape[1]\n",
    "\n",
    "    # Calculate the step size based on the window size and overlap\n",
    "    step_size = window_size - overlap\n",
    "\n",
    "    # Initialize an empty list to store the windows\n",
    "    windows = []\n",
    "\n",
    "    # Slide the window across the data\n",
    "    for i in range(0, num_datapoints - window_size + 1, step_size):\n",
    "        # Extract the window from the data\n",
    "        window = data[:, i:i+window_size]\n",
    "\n",
    "        # Add the window to the list of windows\n",
    "        windows.append(window)\n",
    "\n",
    "    return windows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:55:33.462397Z",
     "start_time": "2024-03-26T21:55:33.459206Z"
    }
   },
   "id": "549154a5cfc8d630",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(16, 2, 5)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = sliding_window(data, window_size, overlap)\n",
    "\n",
    "windows = np.array(windows)\n",
    "windows.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T21:55:54.579004Z",
     "start_time": "2024-03-26T21:55:54.573586Z"
    }
   },
   "id": "a5f68f96cfd29975",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e3ae96bd6dd24442"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/bmis/Documents/AI-Workspace/sEMGClassification/AdaptiveLearning/code'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T05:50:26.766700Z",
     "start_time": "2024-03-27T05:50:26.752123Z"
    }
   },
   "id": "cd130d7f4a3e098a",
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch",
   "language": "python",
   "display_name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
